# coding=utf-8
__author__ = 'nsk'
import re
from bs4 import BeautifulSoup
import nltk

from class_list import Article, Part, Paragraph, Sentence, Ref


def load_file(file_path):
    return open(file_path, 'r')

def process(file_input):
    """
    to extract article and some other message about the article through html format

    including: title, author, highlight(exclusive in papers from ScienceDirect, might be useful in the future), date,
    abstract, keywords, and full text

    :param file_input: file object generated by load_file module
    :return a: a list, a[0] is the title of the article, and a containing dictionaries(title of the part and its
    content, paragraphs could be split by '%%%')
    """

    a = []
    # soup = BeautifulSoup(file_input.read(), 'html5lib')
    soup = BeautifulSoup(file_input.read(), 'html5lib')
    a.append(soup.title.string.strip())
    full_text = []
    index = -1
    for item in soup.find_all('div', id=re.compile('frag_(\d)+')):
        for tag_sec in item.find_all():
            if tag_sec.name == 'h2':
                index += 1
                full_text.append({'title': tag_sec.string, 'content': ''})
            if tag_sec.name == 'p':
                full_text[index]['content'] += '''%%%'''
                for tmp in tag_sec.strings:
                    full_text[index]['content'] += tmp.replace(u'\xa0', ' ')
                    # '\xa0' is the '&nbsp;', which should be removed
    for dic in full_text:
        a.append(dic)
        # print dic['title']
        # print dic['content'].encode('GB18030')  # switch to the next line if on Mac
        # print dic['content']
    return a

def extract_to_tree(a):
    """
    :param a: returned by preprocessing
    :return: an Article instance
    """

    article_ins = Article(a[0])
    for index_part in range(1, len(a)):
        tmp_part = Part()
        tmp_part.index_in_article = index_part - 1
        tmp_part.title_part = a[index_part]['title']
        tmp_para_list = a[index_part]['content'].split('%%%')
        tmp_para_list = list_clean(tmp_para_list)

        # Reference part is handled differently
        if tmp_part.title_part == 'References':
            for index_para in range(0, len(tmp_para_list)):
                tmp_ref = Ref()
                tmp_ref.index_ref = index_para
                tmp_ref.content = tmp_para_list[index_para]
                tmp_part.ref_list.append(tmp_ref)
            article_ins.part_list.append(tmp_part)
            continue

        # handling regular part
        for index_para in range(0, len(tmp_para_list)):
            tmp_para = Paragraph()
            tmp_para.index_in_part = index_para
            tmp_sentence_list = para2sentence(tmp_para_list[index_para])
            for index_sentence in range(0, len(tmp_sentence_list)):
                tmp_sentence = Sentence()
                tmp_sentence.index_in_paragraph = index_sentence
                tmp_sentence.original_sentence = tmp_sentence_list[index_sentence]
                tmp_para.sentence_containing.append(tmp_sentence)
                # 句子分词，求tfisf都还没有
            tmp_part.paragraph_list.append(tmp_para)
        article_ins.part_list.append(tmp_part)
    return article_ins

def para2sentence(para):
    """

    :param para: paragraph as input
    :return: a list, containing sentences as their original form
    """
    # split sentences using tools from nltk
    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
    sents = sent_tokenizer.tokenize(para)

    # fix some unexpected sentence-splitting problems
    sents_new = check_split_validity(sents)

    return sents_new

def check_split_validity(list_input):
    """
    Since sentence splitting results could be imperfect, this function help with fix some of the splitting problems.
    (the appearance of 'i.e.' or 'Fig.' could result in unexpected splitting.)

    :param list_input: Sentence splitting results as list, with the help of
    nltk.data.load('tokenizers/punkt/english.pickle').
    :return:list_output: A new sentence list, merging sentences that are split unexpectedly.
    """
    ept_list = ['fig.', 'eqs.', 'i.e.', 'eq.']  # to store the unexpected cases
    list_output = []
    flg = 0
    for sentence in list_input:
        if flg == 1:
            sentence = tmp_s + sentence
            flg = 0
        for ept in ept_list:
            if ept in sentence[-5:].lower():
                tmp_s = sentence
                flg = 1
                break
        if flg == 0:
            list_output.append(sentence)
    return list_output

def list_clean(list_input):
    """
    to delete empty element in list
    :param list_input:
    :return:
    """
    list_output = []
    for element in list_input:
        if element != '':
            list_output.append(element)
    return list_output

def main():
    file_path = 'article_done/article_0.html'
    article = extract_to_tree(process(load_file(file_path)))
    article.display()



if __name__ == '__main__':
    main()
