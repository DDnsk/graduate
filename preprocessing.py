# coding=utf-8
__author__ = 'nsk'
import re
from bs4 import BeautifulSoup
import nltk

from class_list import Article, Part, Paragraph, Sentence


def load_file(file_path):
    return open(file_path, 'r')

def process(file_input):
    """
    to extract article and some other message about the article through html format

    including: title, author, highlight(exclusive in papers from ScienceDirect, might be useful in the future), date,
    abstract, keywords, and full text

    :param file_input: file object generated by load_file module
    :return a: a list, a[0] is the title of the article, and a containing dictionaries(title of the part and its
    content, paragraphs could be split by '%%%')
    """

    a = []
    # soup = BeautifulSoup(file_input.read(), 'html5lib')
    soup = BeautifulSoup(file_input.read(), 'lxml')
    a.append(soup.title.string.strip())
    full_text = []
    index = -1
    for item in soup.find_all('div', id=re.compile('frag_(\d)+')):
        for tag_sec in item.find_all():
            if tag_sec.name == 'h2':
                index += 1
                full_text.append({'title': tag_sec.string, 'content': ''})
            if tag_sec.name == 'p':
                full_text[index]['content'] += '''%%%'''
                for tmp in tag_sec.strings:
                    full_text[index]['content'] += tmp.replace(u'\xa0', ' ')
                    # '\xa0' is the '&nbsp;', which should be removed
    for dic in full_text:
        a.append(dic)
        # print dic['title']
        # print dic['content'].encode('GB18030')  # switch to the next line if on Mac
        # print dic['content']
    return a

def extract_to_tree(a):
    """
    :param a: returned by preprocessing
    :return:
    """

    article_ins = Article(a[0])
    for index_part in range(1, len(a)):
        tmp_part = Part()
        tmp_part.index_in_article = index_part - 1
        tmp_part.title_part = a[index_part]['title']
        tmp_para_list = a[index_part]['content'].split('%%%')
        tmp_para_list = list_clean(tmp_para_list)
        for index_para in range(0, len(tmp_para_list)):
            tmp_para = Paragraph()
            tmp_para.index_in_part = index_para
            tmp_sentence_list = para2sentence(tmp_para_list[index_para])
            for index_sentence in range(0, len(tmp_sentence_list)):
                tmp_sentence = Sentence()
                tmp_sentence.index_in_paragraph = index_sentence
                tmp_sentence.original_sentence = tmp_sentence_list[index_sentence]
                tmp_para.sentence_containing.append(tmp_sentence)
                # 句子分词，求tfisf都还没有
            tmp_part.paragraph_list.append(tmp_para)
        article_ins.part_list.append(tmp_part)
    article_ins.display()

def para2sentence(para):
    """

    :param para: paragraph as input
    :return: a list, containing sentences as their original form
    """
    sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
    sents = sent_tokenizer.tokenize(para)
    return sents



def list_clean(list_input):
    """
    to delete empty element in list
    :param list_input:
    :return:
    """
    list_output = []
    for element in list_input:
        if element != '':
            list_output.append(element)
    return list_output

def main():
    file_path = 'article_done/article_0.html'
    extract_to_tree(process(load_file(file_path)))



if __name__ == '__main__':
    main()
